---
title: "Multiple Regression Project^[This material is a modification of Dr. Shonda Kuiper's Chapter 3 Car Lab]"
author: "Your Name Here"
date: 'Last compiled: `r format(Sys.time(), "%B %d, %Y at %X")`'
output:
  bookdown::html_document2:
    theme: lumen
    toc: yes
    toc_float: yes
    df_print: kable
---

```{r include = FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", comment = NA, message = FALSE,  warning = FALSE)
library(tidyverse)
library(scales)
```

# How Much is Your Car Worth?

Have you ever browsed through a car dealership and observed the sticker prices of the vehicles? If you have ever seriously considered purchasing a vehicle, you can probably relate to the difficulty of determining if that vehicle is a good deal or not. Most dealerships are willing to negotiate on the sale price, so how can you know how much to negotiate? For novices, it is very helpful to refer to an outside pricing source, such as the Kelley Blue Book, before agreeing on a purchase price.

For over 80 years, the Kelley Blue Book has been a resource for accurate vehicle pricing. Their website, [kbb.com](https://www.kbb.com/), provides a free, on-line resource where anyone can input several car characteristics (such as age, mileage, make, model, and condition) and quickly receive a good estimate of the retail price.

In this project, you will use a relatively small subset of the Kelley Blue Book database to describe the association of several explanatory variables (car characteristics) to the retail value of a car. Before developing a complex multiple regression model with several variables, let's start with a quick review of the simple linear regression model by asking a question: "Are cars with lower mileage worth more?" Clearly, it seems reasonable to expect to see a relationship between mileage (number of miles the car has been driven) and retail value. The dataset, `cardata.csv` stored in the `data` directory of this project, contains the make, model, equipment, mileage and Kelley Blue Book suggested retail price of several used 2005 GM cars.^[Collected using tables from the 2005 Central Edition of the Kelley Blue Book.]

______________

## Preliminary question block{-}

1. Read in the `cardata.csv` file and clean/standardize the variable names using the `clean_names()` function from the `janitor` package.  Store the data in an R object named `cardata` and show the first six rows of `cardata` in a table.  You may download the `cardata.csv` from [https://raw.githubusercontent.com/STAT-ATA-ASU/DATA_MISC/main/cardata.csv](https://raw.githubusercontent.com/STAT-ATA-ASU/DATA_MISC/main/cardata.csv).     

```{r}
# Your code goes here
cardata <- read.csv("https://raw.githubusercontent.com/STAT-ATA-ASU/DATA_MISC/main/cardata.csv")
```

2. Partition `cardata` into a `train` and `test` data sets with roughly 80% of the data in `train` and 20% of the data in `test`.  Use a `set.seed()` value of 49.  Verify the dimensions of your `train` and `test` data sets with the `dim()` function.

```{r}
# Your code goes here
# Partition into train and test data sets
set.seed(49)
index <- sample(1:nrow(cardata), size = 0.80*nrow(cardata), replace = FALSE)
train <- cardata[index, ]

```




3.  Create a scatterplot of `price` versus `mileage` with an included regression line using the `train` data set.  Include an appropriate hyper-linked caption for your scatterplot.

```{r, label = "sp", fig.cap = "Scatterplot of `price` versus `mileage` using the data in `train`"}
# Your code goes here
ggplot(data = train, aes(x = 1, y = 1)) 
```

4.  Does the scatterplot in Figure \@ref(fig:sp) show any relationship between `mileage` and `price`?

**Your text answer here.**

5.  Calculate the least squares regression line to predict `price` from `mileage` using the `train` data set.  Store the results of the regression in `mod_lm` and show output.

```{r}
# Your code here

```


  a.  What is the least squares line (with variables, not $x$ and $y$)? 

**Your answer here.**
  
  b.  Give the $R^2$ value and the strength of the linear relationship. 

**Your answer here.**
  
  c.  Give the $t$-statistics and $p$-values for the estimated intercept and slope.  Test whether $\beta_0$ and $\beta_1$ significantly different from 0? 

**Your answer here.**
  
  d.  Do an intercept and a slope seem like sufficient predictors based on the current $R^2$?  
  
**Your answer here.**
  
6. Compute the residual (the observed suggested retail price minus the expected price calculated from the regression line) for the first car in the `train` data set.  Report the `make ` and `model` of the first car in the `train` data set using inline R code.

```{r}
# Your code here

```

**Your answer here.**
  
7.  What is the largest absolute value of a residual in this regression?

```{r}
# Your code here

```

**Your answer here.**

___________________________

The $t$-statistic for the regression model slope indicates that mileage is an important variable; however, the $R^2$ value and the scatterplot clearly show that the regression line does not explain much of the variation in retail prices. It is always better to take a few minutes to visualize the data instead of solely focusing on a $p$-value. This scatterplot and $R^2$ value suggest that including other explanatory variables in the regression model might help better to explain the variation in retail price.

In this project, you will build a linear combination of explanatory variables that explain the response variable, retail price. As you work through the project, you will find that there is not one technique or "recipe" that will give the best model. In fact, you will come to see that there isn't just one "best" model
for these data.

Multiple regression is arguably the most important method in all of statistics, not only because regression models are so broad in their potential applications, but also because a good understanding of regression is all but essential for understanding so many other, more sophisticated statistical methods. Unlike an assignment for most mathematics classes, where every student is expected to submit the one right answer, it is expected that each final regression model submitted by various students for this project will be at least slightly different. This project focuses on understanding the scientific process of developing a statistical model. It doesnâ€™t matter if you are developing a regression model in economics, psychology, sociology, or engineering, there are always key questions and processes that should be evaluated before a final model is submitted. While a "best" model may not exist for these data, there are certainly many bad models that should be avoided.

______________________

# Multiple Regression Can Serve Multiple Purposes

It is important to note that multiple regression analysis can be used to serve different goals. The most common goals of multiple regression are to

**Describe:** Develop a model to describe the relationship between the explanatory variables and the response variable.

**Predict:** Use a set of sample data to forecast or make predictions. A regression model can be used to predict future response values from explanatory variables observed within the range of our sample data.

**Confirm:** Theories are often developed about which variables, or combination of variables, need to be included in a model. Regression methods can be used to determine if the contribution of each explanatory variable in a model (e.g., mileage) captures much of the variability in the response variable. This includes determining if the association between the explanatory variables and the response could just be due to chance. For example, should mileage be used to predict retail price? Theory may also predict the type of relationship that exists, such as "cars with lower mileage are worth more." More specific theories can also be tested, such as "retail price decreases linearly with
mileage."

In most circumstances, multiple regression analysis is conducted on data from observational studies, not experiments. A significant test statistic for a coefficient can provide evidence that a proposed theoretical relationship exists. However, without a priori theoretical justification, a significant correlation (and thus a significant coefficient) does not imply a causal link between the explanatory variable and the response.

> **Key Concept: Correlation does not imply causation.**

_________________

# Variable Selection Techniques for Description or Prediction Models

Often, the primary issue in multiple regression is determining which variables to include in the model (and which to leave out). Clearly, all potential explanatory variables could be included in a regression model, but that often results in a cumbersome model that is difficult to understand. On the other hand, a model that includes only one or two of the measured explanatory variables may provide substantially different predictions from a complex model. This tension between finding a simple model and finding a model that best explains the response is what makes it difficult to find a "best" model. Finding just the right mix that provides a relatively simple linear combination of explanatory variables often resembles an exploratory artistic process much more than a formulaic recipe. In addition to avoiding an unwieldy model, including redundant or unnecessary variables can result in an inefficient regression model. Inefficiently estimated coefficients have larger variability, so their $t$-statistics may be lower than they should be. Remember that with any multiple comparison problem, an $\alpha$-level of 0.05 means there is a 5% chance that an irrelevant variable will be found significant and may inappropriately be determined important for the model. Failing to include a relevant variable is usually more serious. It can cause biased estimates of the regression coefficients and invalid $t$-statistics, especially when the excluded variable is highly significant or if the excluded variable is correlated with other variables also in the model. For this project, we will consider the response to be the suggested retail price from Kelley Blue Book (the `price` variable in the data). The following variables are considered relevant potential explanatory variables:

* `mileage` (number of miles the car has been driven)
* `make` (Buick, Cadillac, Chevrolet, Pontiac, SAAB, Saturn)
* `model` (specific car for each previously lined `make`)
* `trim` (specific type of `model`)
* `type` (Sedan, Coupe, Hatchback, Convertible or Wagon)
* `cylinder` (number of cylinders: 4, 6, or 8)
* `liter` (a measure of engine size)
* `doors` (number of doors: 2, 4)
* `cruise` (1 = cruise control, 0 = no cruise control)
* `sound` (1 = upgraded speakers, 0 = standard speakers)
* `leather` (1 = leather seats, 0 = not leather seats)

Several statistical techniques are available for choosing between models if the objective of your regression model is to describe a relationship or predict new response variables. For example, a larger $R^2$ value indicates that more of the variation in retail price is explained by the model, but $R^2$ always increases when another predictor is added (this is just an algebraic fact not proven here). $R^2$ is most useful when comparing models with the same number of predictors, but when comparing models with the different numbers of terms (explanatory variables), other techniques are suggested.

When a large number of variables are available, stepwise regression is a sequential variable selection technique that can automate the process of building a model. Stepwise regression is an iterative technique that can be used to identify key variables to include in a regression model. For example, forward stepwise regression begins by fitting several single-predictor regression models for the response, one for each individual explanatory variable. The single explanatory variable with the highest $R^2$ value is then determined to be in the model. In the next step, the first explanatory variable (call it $X_1$) is already in the model, and all possible regression models using $X_1$ and exactly one other explanatory variable are determined. From among these, the regression model with the highest $R^2$ value is again selected to identify $X_2$. After the first and second explanatory variables, $X_1$ and $X_2$, are selected, the process is repeated to
find $X_3$. This continues until no additional variables are significant (typically $\alpha =$ 0.20).

Sequential procedures have a tendency to include too many variables and at the same time they sometimes eliminate important variables. With improvements in technology, most statisticians prefer to use more "global" techniques that compare all possible subsets of the explanatory variables, such as Mallowsâ€™s Cp, Akaikeâ€™s information criterion (AIC), or the Bayesian information criterion (BIC).

_______________

## Stepwise question block{-}


8. Conduct a stepwise regression analysis.  Use `price` as the response variable and `mileage`, `cylinder`, `liter`, `doors`, `cruise`, `sound`, and `leather` as potential explanatory variables.  Use the data set `train`.

  a.  Use forward selection to choose variables for the model.  Report the variables forward selection chooses for the model.
  
```{r}
# Your code here

```
  
  
  b.  Use backward elimination to choose variables for the model.  Report the variables backward elimination retains for the model.
  
```{r}
# Your code here

```
  
9. Use Mallow's $C_p$ to develop a model (`bestsubsetsmodel`).  Specifically, use the package `leaps` in conjunction with the function `regsubsets()` to create a model using Mallows's $C_p$.  The objective is to find the smallest $C_p$ value that is close to the number of coefficients in the model.  Show the results of `plot(bestsubsetsmodel, scale = "Cp")`.  Make sure to use the data set `train` in the `data` argument of `regsubsets()`.

```{r}
# Your code here
library(leaps)

```

10. Compare the regression models from questions (8a), (8b), and (9).

  a.  What explanatory variables are considered important?  **Your answer here.**

  b.  What is $R^2_{\text{adj}}$ for the models?  **Your answer here.**

__________________________

If your goal is to develop a model to describe or predict, it is common to evaluate the strength of the relationship described by the model. Here, we are not concerned about the significance of each explanatory variable but how well the overall model fits. Iterative techniques are useful in providing a high $R^2$ value while limiting the number of variables. If your goal involves confirming a relationship, iterative techniques are not suggested. Confirming a theory is similar to hypothesis testing. Iterative variable selection techniques test each variable, or combination of variables, several times; and thus, the $p$-values are not reliable. The stated significance level for a $t$-statistic is only valid if the data are used for a single test. If multiple tests are conducted to find the best equation, the actual significance level for each test for an individual component is invalid.

> **Key Concept: Iterative techniques should not be used to evaluate the importance of each explanatory variable. The $t$-statistics in the final model may be inflated.**

_______________

# Checking Model Assumptions

## Assessing Shapes and Patterns in Residual Plots

Whenever a regression model is created, it is necessary to check the following model assumptions:

  * The variance of the error population is constant at all levels of the explanatory variables.  In other words, the error terms in the regression model (also called residuals) are assumed to come from a single population with variance $\sigma^2$.
  * The error terms are independent and identically distributed.
  * The error terms follow a normal probability distribution: denoted as $\epsilon_i \sim \mathcal{N}(0, \sigma)$ for $i = 1,\ldots, n$ ($n=$ the total number of observational units in the data).
  
In regression, these assumptions are generally checked by looking at the residuals from the data: $y_i - \hat{y}_i$.  Here, $y_i$  are the observed responses, and $\hat{y}_i$ are the estimated responses calculated from the regression model.  Multi-variable regression equations are difficult to visualize; and even for single-variable (simple) regression lines, residual plots often emphasize violations of model assumptions better than a plot of regression line on a scatterplot.

Instead of conducting formal hypothesis tests about error (i.e, residual values), plots are used to determine visually if the assumptions hold.  The theory and methods are simplest when any scatterplot of residuals resembles a single, horizontal, oval balloon, but real data may not cooperate by confirming to the ideal pattern.  An ornery plot may show a wedge, a curve, or multiple clusters.  Any of these plot patterns reveal that our error terms are violating at least one model assumption, and it is likely that we have inefficient (more variables than necessary) and biased estimates of our model coefficients.  The following section illustrates strategies for dealing with one of these unwanted shapes, a wedge-shaped (or right-opening megaphone) pattern.  

**Heteroscedasticity** is a term used to describe the situation where the variance of the error term is not constant for all levels of the explanatory variables.  For example, in the regression equation, $\widehat{\text{price}} = 24,765 - 0.173 \times \text{mileage}$, the spread of the suggested retail price values around the regression line should be about the same whether mileage is 0 or mileage is 50,000.  If heteroscedasticity exists in the model, the most common remedy is to transform either the explanatory variable, the response variable, or both in the hope that the transformed relationship will exhibit **homoscedasticity** (equal variances) in the error terms.

___________________

## Heteroscedasticity question block{-}

11.  Create a plot of the residuals versus the fitted values for the model with the variables suggested in question (10a) by the $C_p$ statistic.  Start by regressing `price` on the variables suggested in question 10a and store the result in the object `mod`.  Use the data set `train`.

```{r}
# Your Code Here
library(car)

```

12. Describe the standard problem you see in the residuals versus fitted plot.

**Your answer here---There is increasing variance as the fitted values increase.**

13. Use the dataset `cardata` and the verb `mutate()` to create two new response variables: **log(price)** and **$\sqrt{\text{price}}$** with names of `log_price` and `sqrt_price`, respectively in the dataset `cardata`.  Transforming data using square roots, logarithms, or reciprocals can often reduce the heteroscedasticity and skewness.  Regress `log_price` on the same explanatory variables used in `mod` and store the result in `mod_log`.  Regress `sqrt_price` on the same explanatory variables used in `mod` and store the result in `mod_sqrt`.  Make sure to use the data set `train`.


```{r}
# Your code here

```

* Create a residual versus fitted plot for `mod_log` and `mod_sqrt`.

```{r}
# Your code here

```

* Report the $R^2_{\text{adjusted}}$ values for `mod_log` and `mod_sqrt`.

```{r}
# Your code here

```

**Your answer here.**

* Which transformation did the better job of reducing heteroscedasticity and skewness?

**Your answer here.**

_________________

In single variable regression models, residual plots show the same information as the initial fitted line plot. However, the residual plots often emphasize violations of model assumptions better than the fitted line plot. In addition, multivariate regression lines are very difficult to visualize. Thus, residual plots are essential when multiple explanatory variables are used.


## Outliers and Influential Observations

Any data values that donâ€™t seem to fit the general pattern of the dataset are called outliers. If the outlier has an extreme value in the direction of the response variable, the $R^2$ value may be influenced. If the outlier has an extreme value in the direction of an explanatory variable, the model coefficients may be impacted.

__________________

## Outlier questions block{-}

14. What is the `make` and `type` of car for the top right outliers depicted in Figure \@ref(fig:rp)?

**Your answer here.**

```{r, echo = FALSE, label = "rp", fig.cap = "Residuals versus Fitted Values for `mod_log`"}
# Your code here
ggplot(data = train, aes(x = 1, y = 1))
  
```
15.  Create a new regression model named `mod_log_noc` that regresses `log_price` on the same variables used in `mod_log` but excludes Cadillac convertibles.  Start by creating a subset of `train` by excluding convertible Cadillacs (name this subset `train_noc`).  Use `summary()` on both `mod_log_noc` and `mod_log`.  Report both models coefficients.

```{r}
# Your code here

```

**Your answer here.**

_________

If the coefficients change dramatically between the regression models, these points are considered influential. If any observations are influential, great care should be taken to verify their accuracy.  

```{r}
# Your code here

```


* Does this cluster of outliers influence the coefficients of the regression line? **Your answer here.** 

* Which coefficient has the largest percent change? **Your answer here.**



______________

In some situations, clearly understanding outliers can be more time consuming (and possibly more interesting) than working with the rest of the data. It can be quite difficult to determine if an outlier was accurately recorded or to know whether the outliers should be included in the analysis. The simplest approach to this problem is to run the analysis twice: once with the outliers included and once without. If the results are similar, then it doesnâ€™t matter if the outliers are included or not. If the results do change, it is much more difficult to know what to do. Most statisticians tend to err on the side of keeping the outliers in the sample dataset unless there is clear evidence that they were mistakenly recorded. Whatever final model is selected, it is important to state clearly if you are aware that your results are sensitive to outliers.

## Normally Distributed Residuals

To determine if the residuals are normally distributed, two graphs are often created: a histogram of the residuals and a normal probability plot. Normal probability plots are created by sorting the data (the residuals in this case) from smallest to largest. Then, the sorted residual are plotted against a theoretical normal distribution. If the plot forms a straight line, the actual data and the theoretical data have the same shape (i.e. the same distribution).

Checking for normality is most important when using hypothesis tests ($t$-tests) to determine the significance of individual variables. It is not needed to create a regression line or an $R^2$ value. The first four sub-sections above on checking model assumptions are all based on simply looking for patterns in various residual plots. Before a final model is selected, the residuals should be plotted against fitted (estimated) values, observation order, and each explanatory variable in the model. If any patterns exist, it is likely that another model exists that better explains the response variable. At this time, it should be clear that simply plugging data into a software package and using an iterative variable selection technique will not reliably create a "best" model. Even though the calculations of the regression model and $R^2$ do not depend on model assumptions, identifying patterns in residual plots can often lead to another model that better explains the response variable.

> **Key Concept:  Always check residual plots when developing a regression model. If a pattern exists in any of the residual plots, the $R^2$ value is likely to improve if additional terms or transformations are included in the model.**


16.  Regress `log_price` onto `mileage` and store the results in an object named `mod0`.  Create a histogram and a normal probability plot of the residuals.

```{r}
# Your code here
library(broom)

```

* Do the residuals appear to follow a normal distribution?  

**Your answer here.**

______________

## Multicollinearity: Correlation Between Explanatory Variables

**Multicollinearity** exists when the explanatory variables are highly correlated with each other. If two explanatory variables, $X_1$ and $X_2$, are highly correlated, it can be very difficult to identify whether $X_1$, or $X_2$, or both variables are actually responsible for influencing the response variable, $Y$.

___________________

## Multicollinearity question block:{-}

17. Create three regression models using `price` BEFORE it is transformed as the response variable. (Use all data.) In all three models, report the the regression model, $R^2$, the $t$-statistic for the slope coefficients and their corresponding $p$-values.  Make sure to use the `train` data set to create the three models.

    A. In the first model (`mod1`), use only `mileage` and `liter` as explanatory variables. Is `liter` an important explanatory variable in this model?

```{r}
# Your code here

```

**Your answers here for A:**  

* The regression model (`mod1`) is:  

* The $R^2$ value for `mod1` is: 

* The $t$-value and $p$-value 

    B. In the second model (`mod2`), use only `mileage` and `cylinder` as explanatory variables. Is `cylinder` an important explanatory variable in this model?
    
```{r}
# Your code here

```
    
**Your answers here for B:**  

* The regression model (`mod2`) is:  

* The $R^2$ value for `mod2` is: 

* The $t$-value and $p$-value for `cylinder` are: 

C. In the third model (`mod3`), use only `mileage`, `liter`, and `cylinder` as explanatory variables. Is `cylinder` an important explanatory variable in this model?
    
```{r}
# Your code here

```

**Your answers here for C:**  

* The regression model (`mod3`) is:  

* The $R^2$ value for `mod3` is: 

* The $t$-value and $p$-value for `cylinder` are: 

D.  Describe how the coefficient for `liter` depends on whether `cylinder` is in the model.

**Your answer here.**

E.  Create a scatter plot using `ggplot()` of `cylinder` versus `liter` while mapping `type` to color and `make` to shape.  Use `geom_jitter()` instead of `geom_point()`.  Calculate the correlation between these two variables.  How strong is the correlation?

```{r}
# Your code here

```

**Your answer here.**

___________

If multicollinearity exists in a regression model, then the coefficients are not reliable. This is clearly evident for the explanatory variables `liter` and `cylinder` after observing the changes in the estimated regression coefficients occurring from model to model in question (17). If multicollinearity exists, try one of the following approaches:

**Get more information:** If it is possible, expanding the data collection may lead to samples where the variables are not so correlated. Consider whether the data could be collected or measured differently so the variables are not correlated. For example, the data here are only for GM cars. Perhaps the relationship between engine size in liters and the number of cylinders is not so strong for data from across a wider variety of manufacturers.

**Re-evaluate the model:** When two explanatory variables are highly correlated, deleting one variable will not significantly impact the $R^2$ value. However, if there are theoretical reasons to include both variables in the model, keep both terms. In our example, `liter` and number of cylinders (`cylinder`) are measuring essentially the same quantity. Liter represents the volume displaced during one complete engine cycle. The number of cylinders (Cylinder) also is a measure of the volume that can be displaced.

**Combine the variables:** Using other statistical techniques such as _principal components_, it is possible to combine the correlated variables "optimally" into a single variable that can be used in the model. There may be theoretical reasons to combine variables in a certain way. For example, the volume (size) and weight of a car are likely highly positively correlated. Perhaps a new variable defined as `density = weight/volume` could be used in a model predicting price rather than either of these
individual variables.

In this example, we have theoretical reasons to re-evaluate our model. Liter and number of cylinders (`cylinder`) are both measuring displacement (engine size). We will keep `liter` and throw out number of cylinders (`cylinder`), since `liter` is a more specific variable taking on several values (only 4, 6, or 8 cylinder cars appear in the dataset). In general, it may not be possible to "fix" a multicollinearity problem. If the goal is to describe simply or predict retail prices, multicollinearity is not a critical issue. Redundant variables should be eliminated from the model, but highly correlated variables that both contribute to the model would be acceptable if you are not interpreting the coefficients; however, if your goal is to confirm whether an explanatory variable is associated with a response (test a theory), then it is essential to identify the presence of multicollinearity and to recognize that the coefficients are unreliable when it exists.

> **Coefficients (and their $p$-values) are unreliable when multicollinearity exists.**

__________________

# Categorical Explanatory Variables in a Regression Model

If any categorical variables are related to the response variable, then we want to add these variables to our regression model. A common procedure used to incorporate categorical explanatory variables into a regression model is to define dummy variables, also called indicator variables. Creating dummy variables is a process of mapping the one column (variable) of categorical data into several columns (dummy variables) of 0 and 1 data. Using the variable Make as an example, the 5 possible values (`Buick`, `Cadillac`, `Chevrolet`, `Pontiac`, `SAAB`, `Saturn`) can be recoded using 6 dummy variables: one for each of the 6 makes of car. For example, the dummy variable for Buick will have the value 1 for every car that is a Buick and 0 for each car that is not a Buick. Most statistical software packages have a command for creating the dummy variables automatically. Now, any of these dummy variables can be incorporated into a regression model. However, if you want to include Make in its entirety into the model, do not include all 6 dummy variables; 5 will suffice. This is because there is complete redundancy in the sixth dummy variable: knowing the values of the other five variables are all 0 for a particular car automatically tells us that this car belongs to the sixth category. Below, we will leave the Saturn dummy variable out of our model. The coefficient for a dummy variable
is an estimate of the average amount (of the response variable) by which a "1" for that dummy variable will exceed a "0." For example, this will mean that the estimated coefficient for the Buick variable is an estimate of the average change in Price (transformed) when the car is a Buick rather than a Saturn (while all other explanatory variables in the model remain unchanged).

__________________

## Indicator variables question block:{-}

18. Create boxplots using the transformed price as the response variable with `make`, `model`, `trim`, and `type` as the categorical variables. Describe any patterns you see.

```{r}
# Your code here

```

```{r}
# Your code here

```

```{r}
# Your code here

```

```{r}
# Your code here

```

19. Factor variables can explain steps or slope changes that depend on categories. Using `liter`, `doors`, `cruise`, `sound`, `leather`, and `mileage` plus `make` and `type`, create a regression model (name the object `mod_19`) to predict the transformed price (report this model with its output).  Make sure to use the `train` data set in creating `mod_19`. 

```{r}
# Your code here

```

a. Create a normal probability plot of the residuals for `mod_19`.  Does the plot suggest the residuals follow a normal distribution?

```{r}
# Your code here

```

**Your answer here.**

b. Create a residuals versus fitted values plot and a residuals versus order plot.  Does the residuals versus order plot look more random than it did for `mod3` (problem 17 C.)?  Does the normal probability plot of the residuals for `mod3` or `mod_19` look more normal? Display the four plots below to answer the question.

```{r}
# Your code here
library(gridExtra)

```

**Your answer here.**

___________________

The additional categorical variables are important in connecting the residual plots and the model assumptions.

___________________

## Final model question block:{-}

Your final model should accurately predict retail price. It should not have significant clusters, skewness, outliers, or heteroscedasticity appearing in the residual plots. (The typical final model will have an $R^2_{\text{adj}}$ greater than 96%.)

20.  Submit your suggested least squares regression model with appropriate graphs that provide justification for using your model. Discuss any reservations you have.  Make sure you develop your final model (`mod_final`) using the `train` data set.  


```{r}
# Your code here

```

21. Report the test RMSE error for your final model.

```{r}
# Your code here

```

**Your answer here.**

22. Report a 95% prediction interval for a Corvette with 25,000 miles.  Make sure to back transform the interval to the original units (dollars).

```{r}
# Your code here

```


**Your answer here.**

23.  What is the largest absolute value of a residual for your final model (`mod_final`)?  Back transform the answer to be in the same units as the original `price` variable.  How does this compare to the largest absolute value of a residual for `mod_lm` from problem 5?

```{r}
# Your code here

```

**Your answer here.**

____________________